{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6dadd99d-8cf0-46c0-91ad-eb6a29192d0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting nltk\n",
      "  Obtaining dependency information for nltk from https://files.pythonhosted.org/packages/4d/66/7d9e26593edda06e8cb531874633f7c2372279c3b0f46235539fe546df8b/nltk-3.9.1-py3-none-any.whl.metadata\n",
      "  Downloading nltk-3.9.1-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting click (from nltk)\n",
      "  Obtaining dependency information for click from https://files.pythonhosted.org/packages/00/2e/d53fa4befbf2cfa713304affc7ca780ce4fc1fd8710527771b58311a3229/click-8.1.7-py3-none-any.whl.metadata\n",
      "  Downloading click-8.1.7-py3-none-any.whl.metadata (3.0 kB)\n",
      "Requirement already satisfied: joblib in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from nltk) (1.4.2)\n",
      "Collecting regex>=2021.8.3 (from nltk)\n",
      "  Obtaining dependency information for regex>=2021.8.3 from https://files.pythonhosted.org/packages/fc/1b/256ca4e2d5041c0aa2f1dc222f04412b796346ab9ce2aa5147405a9457b4/regex-2024.7.24-cp311-cp311-macosx_11_0_arm64.whl.metadata\n",
      "  Downloading regex-2024.7.24-cp311-cp311-macosx_11_0_arm64.whl.metadata (40 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.5/40.5 kB\u001b[0m \u001b[31m503.0 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting tqdm (from nltk)\n",
      "  Obtaining dependency information for tqdm from https://files.pythonhosted.org/packages/48/5d/acf5905c36149bbaec41ccf7f2b68814647347b72075ac0b1fe3022fdc73/tqdm-4.66.5-py3-none-any.whl.metadata\n",
      "  Downloading tqdm-4.66.5-py3-none-any.whl.metadata (57 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.6/57.6 kB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading nltk-3.9.1-py3-none-any.whl (1.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading regex-2024.7.24-cp311-cp311-macosx_11_0_arm64.whl (278 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m278.9/278.9 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading click-8.1.7-py3-none-any.whl (97 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m97.9/97.9 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading tqdm-4.66.5-py3-none-any.whl (78 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.4/78.4 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: tqdm, regex, click, nltk\n",
      "Successfully installed click-8.1.7 nltk-3.9.1 regex-2024.7.24 tqdm-4.66.5\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.2.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip3.11 install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install nltk\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c69e6208-425d-4299-98ae-408be2b75ddf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                text  \\\n",
      "0                       This is an example sentence.   \n",
      "1  This is another sentence here with numbers 123...   \n",
      "\n",
      "                   processed_text  \n",
      "0                example sentence  \n",
      "1  another sentence number symbol  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import string\n",
    "import re\n",
    "\n",
    "#sample text to show each preprocessing technique\n",
    "data = {'text': [\"This is an example sentence.\", \n",
    "                 \"This is another sentence here with numbers 123 and symbols!?@#%&*$#\"]}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "#a manual stop words list was created as there was an error downloading it from nltk\n",
    "stopWords = [\n",
    "    'i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \n",
    "    \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', \n",
    "    'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', \n",
    "    'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', \n",
    "    'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', \n",
    "    'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', \n",
    "    'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', \n",
    "    'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', \n",
    "    'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', \n",
    "    'with', 'about', 'against', 'between', 'into', 'through', 'during', \n",
    "    'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', \n",
    "    'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', \n",
    "    'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', \n",
    "    'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', \n",
    "    'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', \n",
    "    'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \n",
    "    \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', \n",
    "    're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", \n",
    "    'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \n",
    "    \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \n",
    "    \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \n",
    "    \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \n",
    "    \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"\n",
    "]\n",
    "#converting strings to lowercase\n",
    "df['text_lower'] = df['text'].str.lower()\n",
    "\n",
    "#removing punctuation\n",
    "df['text_no_punct'] = df['text_lower'].str.translate(str.maketrans('', '', string.punctuation))\n",
    "\n",
    "#removign numbers\n",
    "df['text_no_numbers'] = df['text_no_punct'].str.replace(r'\\d+', '', regex=True)\n",
    "\n",
    "#creating word tokens\n",
    "df['tokens'] = df['text_no_numbers'].str.split()\n",
    "\n",
    "#removing stopwords\n",
    "df['filtered_tokens'] = df['tokens'].apply(lambda x: [word for word in x if word not in stopWords])\n",
    "\n",
    "#making words to their base form\n",
    "def simple_stem(word):\n",
    "    suffixes = ('ing', 'ly', 'ed', 'ious', 'ies', 'ive', 'es', 's', 'ment')\n",
    "    for suffix in suffixes:\n",
    "        if word.endswith(suffix):\n",
    "            return word[:-len(suffix)]\n",
    "    return word\n",
    "\n",
    "df['stemmed_tokens'] = df['filtered_tokens'].apply(lambda x: [simple_stem(word) for word in x])\n",
    "\n",
    "# Lemmatization\n",
    "# df['lemmatized_tokens'] = df['filtered_tokens'].apply(lambda x: [lemmatizer.lemmatize(word) for word in x]\n",
    "df['processed_text'] = df['stemmed_tokens'].apply(lambda x: \" \".join(x))\n",
    "print(df[['text', 'processed_text']])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "420c9b13-d9d8-41e2-ba74-acfba05ccfb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  label                                               text\n",
      "0   ham  Go until jurong point, crazy.. Available only ...\n",
      "1   ham                      Ok lar... Joking wif u oni...\n",
      "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
      "3   ham  U dun say so early hor... U c already then say...\n",
      "4   ham  Nah I don't think he goes to usf, he lives aro...\n",
      "  label                                     processed_text\n",
      "0   ham  go jurong point crazy available bugis n great ...\n",
      "1   ham                            ok lar joking wif u oni\n",
      "2  spam  free entry wkly comp win fa cup final tkts st ...\n",
      "3   ham                u dun say early hor u c already say\n",
      "4   ham        nah dont think goes usf lives around though\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import string\n",
    "import re\n",
    "\n",
    "file_path = 'SMSSpamCollection'\n",
    "df = pd.read_csv(file_path, sep='\\t', header=None, names=['label', 'text'])\n",
    "print(df.head())\n",
    "\n",
    "stopWords = [\n",
    "    'i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you',\n",
    "    \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself',\n",
    "    'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her',\n",
    "    'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them',\n",
    "    'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom',\n",
    "    'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are',\n",
    "    'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having',\n",
    "    'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if',\n",
    "    'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for',\n",
    "    'with', 'about', 'against', 'between', 'into', 'through', 'during',\n",
    "    'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down',\n",
    "    'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further',\n",
    "    'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how',\n",
    "    'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other',\n",
    "    'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so',\n",
    "    'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don',\n",
    "    \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o',\n",
    "    're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\",\n",
    "    'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn',\n",
    "    \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn',\n",
    "    \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan',\n",
    "    \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren',\n",
    "    \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"\n",
    "]\n",
    "\n",
    "#Preprocessing\n",
    "df['text_lower'] = df['text'].str.lower()\n",
    "df['text_no_punct'] = df['text_lower'].str.translate(str.maketrans('', '', string.punctuation))\n",
    "df['text_no_numbers'] = df['text_no_punct'].str.replace(r'\\d+', '', regex=True)\n",
    "df['tokens'] = df['text_no_numbers'].str.split()\n",
    "df['filtered_tokens'] = df['tokens'].apply(lambda x: [word for word in x if word not in stopWords])\n",
    "df['processed_text'] = df['filtered_tokens'].apply(lambda x: \" \".join(x))\n",
    "\n",
    "print(df[['label', 'processed_text']].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05047234-5115-403a-9cee-50dede6db3b6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
